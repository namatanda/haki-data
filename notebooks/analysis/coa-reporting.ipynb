{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Any, List, Union, Optional\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/fiend/Documents/coa/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', 45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_nan_columns(df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Drop rows containing NaN values from the specified columns of a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame to process.\n",
    "        columns (List[str]): A list of column names to check for NaN values.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The updated DataFrame with NaN-containing rows dropped.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If any of the specified columns are not present in the DataFrame.\n",
    "    \"\"\"\n",
    "    # Validate that all specified columns exist in the DataFrame\n",
    "    missing_columns = set(columns) - set(df.columns)\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Columns not found in DataFrame: {', '.join(missing_columns)}\")\n",
    "\n",
    "    # Identify columns with NaN values\n",
    "    nan_columns = df[columns].columns[df[columns].isna().any()].tolist()\n",
    "\n",
    "    # Log dropped rows if any\n",
    "    if nan_columns:\n",
    "        nan_count = df[columns].isna().sum()\n",
    "        logger.info(\"Dropping rows with NaN values:\")\n",
    "        for col in nan_columns:\n",
    "            logger.info(f\"  {col}: {nan_count[col]} rows\")\n",
    "\n",
    "    # Drop rows with NaN values in specified columns\n",
    "    original_row_count = len(df)\n",
    "    df_cleaned = df.dropna(subset=columns)\n",
    "    dropped_row_count = original_row_count - len(df_cleaned)\n",
    "\n",
    "    if dropped_row_count > 0:\n",
    "        logger.info(f\"Total rows dropped: {dropped_row_count}\")\n",
    "    else:\n",
    "        logger.info(\"No rows were dropped.\")\n",
    "\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove duplicates from a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): Input DataFrame.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with duplicates removed.\n",
    "    \"\"\"\n",
    "    num_duplicates = data.duplicated().sum()\n",
    "    \n",
    "    if num_duplicates > 0:\n",
    "        logging.info(f\"{num_duplicates} duplicates found.\")\n",
    "        data = data.drop_duplicates(keep=\"first\").reset_index(drop=True)\n",
    "        logging.info(f\"{num_duplicates} duplicates removed.\")\n",
    "    else:\n",
    "        logging.info(\"No duplicates found.\")\n",
    "    \n",
    "    # check and drop duplicates on 'court', 'date_dd', 'date_mon', 'date_yyyy', 'caseid_type', 'caseid_no', 'filed_dd', 'filed_mon', 'filed_yyyy', 'comingfor', 'outcome'\n",
    "    duplicates = data.duplicated(subset=['court', 'date_dd', 'date_mon', 'date_yyyy', 'caseid_type', 'caseid_no', 'filed_dd', 'filed_mon', 'filed_yyyy', 'comingfor', 'outcome'], keep=False).sum()\n",
    "    \n",
    "    if duplicates > 0:\n",
    "        logging.info(f\"{duplicates} duplicates found on 'court', 'date_dd', 'date_mon', 'date_yyyy', 'caseid_type', 'caseid_no', 'filed_dd', 'filed_mon', 'filed_yyyy', 'comingfor', 'outcome'.\")\n",
    "        data = data.drop_duplicates(subset=['court', 'date_dd', 'date_mon', 'date_yyyy', 'caseid_type', 'caseid_no', 'filed_dd', 'filed_mon', 'filed_yyyy', 'comingfor', 'outcome'], keep=False)\n",
    "        logging.info(f\"{duplicates} duplicates removed on 'court', 'date_dd', 'date_mon', 'date_yyyy', 'caseid_type', 'caseid_no', 'filed_dd', 'filed_mon', 'filed_yyyy', 'comingfor', 'outcome'.\")\n",
    "    else:  \n",
    "        logging.info(\"No duplicates found on 'court', 'date_dd', 'date_mon', 'date_yyyy', 'caseid_type', 'caseid_no', 'filed_dd', 'filed_mon', 'filed_yyyy', 'comingfor', 'outcome'.\")\n",
    "\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_date_column(df: pd.DataFrame, column_names: List[str], new_col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a new date column in the DataFrame by concatenating the values of three specified columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        column_names (List[str]): A list of three column names to be concatenated [year, month, day].\n",
    "        new_col (str): The import commandsname of the new date column to be created.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with the new date column added.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the input list doesn't contain exactly three column names or if columns are missing.\n",
    "    \"\"\"\n",
    "    if len(column_names) != 3:\n",
    "        raise ValueError(\"column_names must contain exactly three elements: [year, month, day]\")\n",
    "\n",
    "    year_col, month_col, day_col = column_names\n",
    "\n",
    "    # Check if all required columns exist in the DataFrame\n",
    "    missing_columns = set(column_names) - set(df.columns)\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing columns in DataFrame: {', '.join(missing_columns)}\")\n",
    "\n",
    "    # Create copies to avoid SettingWithCopyWarning\n",
    "    df = df.copy()\n",
    "\n",
    "    try:\n",
    "        # Convert year and day columns to integers\n",
    "        df[year_col] = df[year_col].astype(float).astype(int)\n",
    "        df[day_col] = df[day_col].astype(float).astype(int)\n",
    "\n",
    "        # Concatenate the columns to create a date string\n",
    "        df[new_col] = (df[year_col].astype(str) + '-' + \n",
    "                       df[month_col].astype(str) + '-' + \n",
    "                       df[day_col].astype(str))\n",
    "\n",
    "        # Convert to datetime\n",
    "        df[new_col] = pd.to_datetime(df[new_col], errors='coerce')\n",
    "\n",
    "        # Log information about the conversion\n",
    "        valid_dates = df[new_col].notna().sum()\n",
    "        logger.info(f\"Created new date column '{new_col}'. Valid dates: {valid_dates}/{len(df)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating date column: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove leading and trailing spaces to the add title case to all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_title_case(text):\n",
    "    \"\"\"\n",
    "    Apply title case to a given string.\n",
    "    \n",
    "    Args:\n",
    "        text: The input string to process.\n",
    "    \n",
    "    Returns:\n",
    "        str: The processed string in title case.\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return np.nan\n",
    "    if not isinstance(text, str):\n",
    "        logger.warning(f\"Non-string value encountered: {text}\")\n",
    "        return str(text)\n",
    "    return text.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_outcome_column(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process the 'outcome' column of the DataFrame by applying title case.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing the 'outcome' column.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with the processed 'outcome' column.\n",
    "    \"\"\"\n",
    "    if 'outcome' not in df.columns:\n",
    "        logger.error(\"'outcome' column not found in the DataFrame\")\n",
    "        return df\n",
    "\n",
    "    original_null_count = df['outcome'].isnull().sum()\n",
    "    \n",
    "    df['outcome'] = df['outcome'].apply(apply_title_case)\n",
    "    \n",
    "    new_null_count = df['outcome'].isnull().sum()\n",
    "    if new_null_count > original_null_count:\n",
    "        logger.warning(f\"Number of null values in 'outcome' increased from {original_null_count} to {new_null_count}\")\n",
    "    \n",
    "    non_string_count = df['outcome'].apply(lambda x: not isinstance(x, str) if pd.notna(x) else False).sum()\n",
    "    if non_string_count > 0:\n",
    "        logger.warning(f\"Found {non_string_count} non-string values in 'outcome' after processing\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_null_values(df: pd.DataFrame, column_name: str = 'outcome') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Drop rows from the DataFrame where the specified column contains null values.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame from which to drop rows.\n",
    "        column_name (str): The name of the column to check for null values. Default is 'outcome'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with rows containing null values in the specified column dropped.\n",
    "    \"\"\"\n",
    "    initial_row_count: int = df.shape[0]\n",
    "    cleaned_df: pd.DataFrame = df.dropna(subset=[column_name])\n",
    "    final_row_count: int = cleaned_df.shape[0]\n",
    "    dropped_row_count: int = initial_row_count - final_row_count\n",
    "    \n",
    "    logging.info(f\"Total dropped rows with null values in '{column_name}': {dropped_row_count}\")\n",
    "    \n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_case(case_type: str, criminal_cases: Optional[List[str]]) -> str:\n",
    "    \"\"\"\n",
    "    Categorize a case as 'Criminal' or 'Civil' based on its type.\n",
    "    \n",
    "    Args:\n",
    "        case_type (str): The type of the case.\n",
    "        criminal_cases (Optional[List[str]]): List of case types considered as criminal.\n",
    "        \n",
    "    Returns:\n",
    "        str: 'Criminal' if the case type is in the criminal cases list or if criminal_cases is None, 'Civil' otherwise.\n",
    "    \"\"\"\n",
    "    if criminal_cases is None:\n",
    "        return 'Criminal'\n",
    "    else:\n",
    "        return 'Criminal' if case_type in criminal_cases else 'Civil'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def categorize_cases(df: pd.DataFrame, criminal_cases: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Categorize all cases in the DataFrame as 'Criminal' or 'Civil'.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing case data.\n",
    "        criminal_cases (Optional[List[str]]): List of case types considered as criminal.\n",
    "            If None, all cases are categorized as 'Criminal'.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with an added 'nature' column indicating case nature.\n",
    "    \"\"\"\n",
    "    df['nature'] = df['case_type'].apply(lambda x: categorize_case(x, criminal_cases))\n",
    "\n",
    "    # Check for presence of both case types\n",
    "    if 'Criminal' not in df['nature'].values:\n",
    "        logging.warning(\"No criminal cases found in the DataFrame.\")\n",
    "    if 'Civil' not in df['nature'].values:\n",
    "        logging.warning(\"No civil cases found in the DataFrame.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create case types using the long format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_dict(value: Any, dictionary: Dict[str, Union[str, List[Any]]]) -> Union[str, None]:\n",
    "    \"\"\"\n",
    "    Find all keys in a dictionary where the given value matches.\n",
    "\n",
    "    Args:\n",
    "        value: The value to search for.\n",
    "        dictionary: The dictionary to search in.\n",
    "\n",
    "    Returns:\n",
    "        A list of keys where the value matches, or None if no matches are found.\n",
    "    \"\"\"\n",
    "    matching_keys = []\n",
    "    for key, dict_value in dictionary.items():\n",
    "        if isinstance(dict_value, str) and dict_value == value:\n",
    "            matching_keys.append(key)\n",
    "        elif isinstance(dict_value, list) and value in dict_value:\n",
    "            matching_keys.append(key)\n",
    "    \n",
    "    if not matching_keys:\n",
    "        return None\n",
    "    elif len(matching_keys) == 1:\n",
    "        return matching_keys[0]\n",
    "    else:\n",
    "        return matching_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Case number as a unique identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_case_num(df: pd.DataFrame, court_col: str, caseid_type_col: str, caseid_no_col: str, filed_date, new_col='case_number') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates a case number by concatenating court, caseid_type, caseid_no, and filed_yyyy columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe containing the necessary columns.\n",
    "        court_col (str): The name of the column containing court information.\n",
    "        caseid_type_col (str): The name of the column containing case ID type.\n",
    "        caseid_no_col (str): The name of the column containing case ID number.\n",
    "        filed_yyyy_col (str): The name of the column containing the year the case was filed.\n",
    "        new_col (str): The name of the new column to be created for the case number. Default is 'case_num'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with the new case number column.\n",
    "    \"\"\"\n",
    "    # drop if any if court_col: str, caseid_type_col: str, caseid_no_col: str, filed_yyyy_col: str are null and log warning message \n",
    "    if pd.isna(court_col) or pd.isna(caseid_type_col) or pd.isna(caseid_no_col) or pd.isna(filed_date):\n",
    "        logging.warning(\"One or more columns are null and will be dropped.\")\n",
    "        df.dropna(subset=[court_col, caseid_type_col, caseid_no_col, filed_date], inplace=True)\n",
    "        \n",
    "\n",
    "    df[new_col] = df[court_col] + '/' + df[caseid_type_col] + '/' + df[caseid_no_col] + '/' + df[filed_date].astype(str)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filed cases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_concluded(outcome: str, resolved_outcomes: List[str]) -> int:\n",
    "    \"\"\"\n",
    "    Determine if the case is concluded based on the outcome.\n",
    "    \n",
    "    Args:\n",
    "        outcome (str): The outcome of the case.\n",
    "        resolved_outcomes (List[str]): List of outcomes considered as resolved.\n",
    "        \n",
    "    Returns:\n",
    "        int: 1 if the case outcome is resolved, 0 otherwise.\n",
    "    \"\"\"\n",
    "    return 1 if outcome in resolved_outcomes else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_case_registered(outcome: str, activity_date: Union[pd.Timestamp, str], filed_date: Union[pd.Timestamp, str]) -> bool:\n",
    "    \"\"\"\n",
    "    Determine if a case is registered based on its outcome and dates.\n",
    "\n",
    "    Args:\n",
    "        outcome (str): The outcome of the case.\n",
    "        activity_date (Union[pd.Timestamp, str]): The date of the activity.\n",
    "        filed_date (Union[pd.Timestamp, str]): The date the case was filed.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the case is registered, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Normalize the outcome string\n",
    "        normalized_outcome = outcome.strip().lower()\n",
    "\n",
    "        # Check if the outcome indicates registration\n",
    "        is_registered_outcome = 'case registered/filed' in normalized_outcome \n",
    "\n",
    "        # Convert dates to pd.Timestamp if they're strings\n",
    "        if isinstance(activity_date, str):\n",
    "            activity_date = pd.to_datetime(activity_date, errors='coerce')\n",
    "        if isinstance(filed_date, str):\n",
    "            filed_date = pd.to_datetime(filed_date, errors='coerce')\n",
    "\n",
    "        # Check if dates are equal\n",
    "        dates_match = pd.notna(activity_date) and pd.notna(filed_date) and activity_date == filed_date\n",
    "\n",
    "        is_registered = is_registered_outcome and dates_match\n",
    "\n",
    "        if is_registered:\n",
    "            logger.debug(f\"Case registered: outcome='{outcome}', activity_date={activity_date}, filed_date={filed_date}\")\n",
    "        \n",
    "        return is_registered\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in is_case_registered: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply is_case_registered to the DataFrame\n",
    "def process_case_status(df: pd.DataFrame, resolved_outcome: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process the DataFrame to add 'concluded' and 'registered' columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing case information.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with added 'concluded' and 'registered' columns.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If required columns are missing from the DataFrame.\n",
    "    \"\"\"\n",
    "    required_columns = ['outcome', 'activity_date', 'filed_date']\n",
    "    missing_columns = set(required_columns) - set(df.columns)\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {', '.join(missing_columns)}\")\n",
    "\n",
    "    #df['concluded'] = df['outcome'].apply(is_case_concluded)\n",
    "    df['concluded'] = df['outcome'].apply(lambda x: is_concluded(x, resolved_outcome))\n",
    "    df['registered'] = df.apply(lambda row: is_case_registered(row['outcome'], row['activity_date'], row['filed_date']), axis=1)\n",
    "\n",
    "    logger.info(f\"Processed {len(df)} cases\")\n",
    "    logger.info(f\"Concluded cases: {df['concluded'].sum()}\")\n",
    "    logger.info(f\"Registered cases: {df['registered'].sum()}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total filed/resolved  per court cases by case_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_court_outcomes(df: pd.DataFrame, start_date: str, end_date: str, outcome: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the number of case outcomes per court within a specified period.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): A pandas DataFrame containing the data.\n",
    "        start_date (str): The starting date of the period (YYYY-MM-DD format).\n",
    "        end_date (str): The ending date of the period (YYYY-MM-DD format).\n",
    "        outcome (str): A column representing the outcome of interest.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame showing the number of resolved cases per court and case category.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        period_start = pd.to_datetime(start_date)\n",
    "        period_end = pd.to_datetime(end_date)\n",
    "        \n",
    "        if period_start > period_end:\n",
    "            raise ValueError(\"start_date must be earlier than end_date\")\n",
    "        \n",
    "        required_columns = {'court', 'case_type', 'activity_date', outcome}\n",
    "        if not required_columns.issubset(df.columns):\n",
    "            missing_columns = required_columns - set(df.columns)\n",
    "            raise KeyError(f\"Missing required columns: {missing_columns}\")\n",
    "        \n",
    "        filtered_cases = df[\n",
    "            (df['activity_date'] >= period_start) &\n",
    "            (df['activity_date'] <= period_end) &\n",
    "            (df[outcome] == 1)\n",
    "        ]\n",
    "        \n",
    "        if filtered_cases.empty:\n",
    "            logging.warning(\"No cases found for the given date range and outcome.\")\n",
    "   \n",
    "        outcome_by_type = (\n",
    "            filtered_cases\n",
    "            .groupby(['court', 'case_type'])\n",
    "            .size()\n",
    "            .reset_index(name='num_cases')\n",
    "        )\n",
    "\n",
    "        result = outcome_by_type.pivot_table(\n",
    "            index='court', \n",
    "            columns='case_type', \n",
    "            values='num_cases', \n",
    "            fill_value=0\n",
    "        )\n",
    "        \n",
    "        logging.info(\"Successfully calculated case outcomes per court.\")\n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_time_limit(age, case_category, concluded, time_lines):\n",
    "    \"\"\"\n",
    "    Check if a case falls within the specified time limit for its category and is concluded.\n",
    "    \n",
    "    Parameters:\n",
    "        age (int): The age of the case in days.\n",
    "        case_category (str): The category of the case.\n",
    "        concluded (int): The status of the case conclusion (1 for concluded, 0 otherwise).\n",
    "        time_lines (dict): A dictionary with case categories as keys and time limits as values.\n",
    "        \n",
    "    Returns:\n",
    "        int: 1 if the case is within the time limit and concluded, otherwise 0.\n",
    "    \"\"\"\n",
    "    time_limit = time_lines.get(case_category, 0)\n",
    "    return 1 if age <= time_limit and concluded == 1 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MERIT_OUTCOMES = [\n",
    "    'Ruling Delivered- Case Closed', \n",
    "    'Judgment Delivered- Case Closed',\n",
    "    'Judgment Delivered',\n",
    "    'Judgment Delivered- Acquittal',\n",
    "    'Judgment Delivered- Convicted',\n",
    "    'Grant Revoked',\n",
    "    'Ruling Delivered- Accused Discharged',\n",
    "    'Retrial'\n",
    "]\n",
    "\n",
    "MERIT_CATEGORY = {\n",
    "    'Judgment Delivered': [\n",
    "        'Judgment Delivered- Case Closed',\n",
    "        'Judgment Delivered',\n",
    "        'Judgment Delivered- Acquittal',\n",
    "        'Judgment Delivered- Convicted',\n",
    "        'Grant Revoked',\n",
    "        'Retrial'\n",
    "        ],\n",
    "    'Ruling Case Closed': [\n",
    "        'Ruling Delivered- Case Closed', \n",
    "        'Ruling Delivered- Accused Discharged',\n",
    "        ],\n",
    "    'Final Grant': [\n",
    "        'Grant Confirmed',\n",
    "        'Limited Grant Issued',\n",
    "        ],\n",
    "    'Case Withdrawn': [\n",
    "        'Matter Withdrawn',\n",
    "        'Application Withdrawn - Case Closed',\n",
    "        ],\n",
    "   'Out Of Court Settlement': [\n",
    "        'Consent Recorded - Case Closed',\n",
    "        'Matter Settled Through Mediation',\n",
    "        'Out Of Court Settlement Reached',\n",
    "    ],\n",
    "    'Dismissed':[\n",
    "        'Dismissed For Want Of Prosecution - Case Closed',\n",
    "        'Dismissed',\n",
    "        'Appeal Dismissed',\n",
    "        'Terminated'\n",
    "    ],\n",
    "    'Case Transfered': [\n",
    "        'File Transfered -case Closed',\n",
    "        'File Transferred',\n",
    "    ],\n",
    "    'Case Closed': [\n",
    "        'Struck Out',\n",
    "        'Application Dismissed - Case Closed',\n",
    "        'Application Allowed - Case Closed',\n",
    "        'Matter Settled- Case Closed',\n",
    "        'Ruling Delivered- Application Closed',\n",
    "        'Consolidated- Case Closed',\n",
    "        'Abated',\n",
    "        'Placed In Probation',\n",
    "        'Revision Declined',\n",
    "        'Probation Orders Issued',\n",
    "        'Appeal Rejected',\n",
    "        'Interlocutory Judgement Entered',\n",
    "    ],\n",
    "}\n",
    "\n",
    "CRIMINAL_CASES = [\n",
    "    'Murder Case',\n",
    "    'Criminal Revision',\n",
    "    'Criminal Appeal',\n",
    "    'Murder - Gender Justice Criminal Case',\n",
    "    'Criminal Court Martial Appeal',\n",
    "    'Anti-Corruption and Economic Crimes Revision',\n",
    "    'Criminal Miscellaneous Application',\n",
    "    'Criminal Applications', \n",
    "    'COA Criminal Appeal'\n",
    "]\n",
    "\n",
    "RESOLVED_OUTCOMES = [\n",
    "    'Ruling Delivered- Case Closed', 'Grant Confirmed', 'Matter Withdrawn',\n",
    "    'Dismissed For Want Of Prosecution - Case Closed', 'Dismissed',\n",
    "    'Terminated', 'Judgment Delivered- Case Closed',\n",
    "     'Application Allowed - Case Closed',\n",
    "    'Matter Settled- Case Closed', 'Consent Recorded - Case Closed',\n",
    "    'Judgment Delivered', 'Judgment Delivered- Acquittal',\n",
    "    'Judgment Delivered- Convicted', 'Application Withdrawn - Case Closed',\n",
    "    'Struck Out', 'Application Dismissed - Case Closed',\n",
    "    'Out Of Court Settlement Reached', 'Terminated',\n",
    "    'Ruling Delivered- Application Closed', 'Consolidated- Case Closed',\n",
    "    'Interlocutory Judgement Entered', 'Abated', 'Limited Grant Issued',\n",
    "    'Grant Revoked', 'Placed In Probation', 'Ruling Delivered- Accused Discharged',\n",
    "    'Revision Declined', 'Retrial', 'Probation Orders Issued',\n",
    "    'Matter Settled Through Mediation', 'Appeal Dismissed', 'Appeal Rejected', \n",
    "    'Order Issued - Case Closed',\n",
    "    'Terminated/ Struck Out/ Dismissed/Case Closed '\n",
    "]\n",
    "\n",
    "COA_PMMU_TIME_LINES = {\n",
    "    'COA  Criminal Appeal': 360,\n",
    "    'Civil Appeal': 360,\n",
    "    'Civil Applications': 90,\n",
    "    'Criminal Applications': 90,\n",
    "}\n",
    "\n",
    "NON_ADJOURNABLE = [\n",
    "    'Taxation and Issuance of Certificates',\n",
    "    'Orders',\n",
    "    'Appointments of  Mediator',\n",
    "    'Screening of files for Mediation',\n",
    "    'Post-judgment',\n",
    "    'Re-activation',\n",
    "    'Reactivation',\n",
    "    'Notice of Taxation',\n",
    "    'Entering Interlocutory Judgments',\n",
    "    'Approval by DR', \n",
    "    'Registration/Filing-Application', \n",
    "    'Registration/Filing', \n",
    "    'Registration/Filing-Application',\n",
    " ]\n",
    "\n",
    "TRANSFERED_OUTCOMES = ['File Transfered -case Closed', 'File Transferred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/fiend/Documents/coa/CoA/coa_data-23-24.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['court_name'] = df['court_name'].replace('Malindi Court of Appeal_Court of Appeal', 'Mombasa Court of Appeal_Court of Appeal')\n",
    "df = df.rename(columns={'court_name': 'court'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(f'{file_path}/q2-24-25-coa.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create activity date and filed date columns\n",
    "df = create_date_column(df.copy(), ['date_dd', 'date_mon', 'date_yyyy'], 'activity_date')\n",
    "df = create_date_column(df.copy(), ['filed_dd', 'filed_mon', 'filed_yyyy'], 'filed_date')\n",
    "\n",
    "start_date = pd.to_datetime('2023-07-01')\n",
    "cutoff_date = pd.to_datetime('2024-06-30')\n",
    "\n",
    "df = df[df['activity_date'] <= cutoff_date]\n",
    "\n",
    "df = drop_nan_columns(df, ['date_dd', 'date_mon', 'date_yyyy', 'caseid_type', 'caseid_no',\n",
    "       'filed_dd', 'filed_mon', 'filed_yyyy', 'case_type', 'comingfor'])\n",
    "\n",
    "df = remove_duplicates(df)\n",
    "\n",
    "#Apply title case to the outcomes column\n",
    "df = process_outcome_column(df)\n",
    "\n",
    "# missing outcomes\n",
    "df = drop_null_values(df)\n",
    "\n",
    "# Add broad case category of civil and criminal\n",
    "df = categorize_cases(df, CRIMINAL_CASES)\n",
    "\n",
    "# Add case number to the data\n",
    "df = generate_case_num(df, 'court', 'caseid_type', 'caseid_no', 'filed_date')\n",
    "df = df.sort_values(by=['activity_date', 'case_number'])\n",
    "\n",
    "# Remove whitespace and rename terminated \n",
    "df['outcome'] = df['outcome'].str.strip()\n",
    "df['outcome'] = df['outcome'].replace('Terminated/ Struck Out/ Dismissed/Case Closed ', 'Terminated')\n",
    "\n",
    "# Add filed and resolved outcomes\n",
    "df = process_case_status(df, RESOLVED_OUTCOMES)\n",
    "\n",
    "# Apply the function to create a new column with keys\n",
    "df['productivity'] = df['outcome'].apply(lambda x: apply_dict(x, MERIT_CATEGORY))\n",
    "\n",
    "# Add the age column \n",
    "df['age'] = (df['activity_date'] - df['filed_date']).dt.days\n",
    "\n",
    "# Add time lines\n",
    "df['time_lines'] = df.apply(lambda row: check_time_limit(row['age'], row['case_type'], row['concluded'], COA_PMMU_TIME_LINES), axis=1)\n",
    "\n",
    "\n",
    "# remove leading and trailing spaces on comingfor\n",
    "df['comingfor'] = df['comingfor'].str.strip()\n",
    "\n",
    "# create a new column of 1 if reason_adj is not null and comingfor is not in non_adjourned else 0\n",
    "df['adjourned'] = (df['reason_adj'].notnull() & df['comingfor'].apply(lambda x: x not in NON_ADJOURNABLE)).astype(int)\n",
    "# an event is adjournable if non_adjourned is not in comingfor\n",
    "df['adjournable'] = df['comingfor'].apply(lambda x: x not in NON_ADJOURNABLE).astype(int)\n",
    "# Extract the quarter from the activity_date column\n",
    "df['quarter'] = df['activity_date'].dt.to_period('Q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data for output\n",
    "filed_cases = analyze_court_outcomes(df, start_date, cutoff_date, 'registered')\n",
    "resolved_cases = analyze_court_outcomes(df, start_date, cutoff_date, 'concluded')\n",
    "\n",
    "monthly_filed_cases = df.groupby(['court','date_mon']).agg({'registered':'sum'}).reset_index()\n",
    "monthly_concluded_cases = df.groupby(['court','date_mon']).agg({'concluded':'sum'}).reset_index()\n",
    "\n",
    "average_time_to_conclude = df.loc[df['concluded'] == 1].pivot_table(index='court', columns='nature', values='age', aggfunc='mean', fill_value=0).round(2)\n",
    "\n",
    "\n",
    "pmmu_timelines = df[df['time_lines'] == 1].pivot_table(index='court', columns='case_type', values='time_lines', aggfunc='count', fill_value=0)\n",
    "total_concluded_per_court = df[df['concluded'] == 1].pivot_table(index='court', columns='case_type', values='time_lines', aggfunc='count', fill_value=0)\n",
    "resolved_within_pmmu_timeline = pmmu_timelines / total_concluded_per_court\n",
    "\n",
    "court_productivity = df.pivot_table(index='court', columns='productivity', values='concluded', aggfunc='count', fill_value=0)\n",
    "court_productivity = court_productivity.rename_axis(columns=None)\n",
    "# judge_productivity = judge_df.pivot_table(index='judge_1', columns='productivity', values='concluded', aggfunc='sum', fill_value=0)\n",
    "# judge_productivity = judge_productivity.rename_axis(columns=None)\n",
    "\n",
    "adjourned_per_court = df.groupby(['court', 'reason_adj'])['adjourned'].sum().reset_index(name='count')\n",
    "adjourned = df.groupby('court')['adjourned'].sum().reset_index(name='total_adjourned')\n",
    "adjournable = df.groupby('court')['adjournable'].sum().reset_index(name='total_adjournable')\n",
    "# deternine the rate of adjournments\n",
    "adjourn_proportion = pd.merge(adjourned, adjournable, on=['court'])\n",
    "adjourn_proportion['adjourn_proportion'] = (adjourn_proportion['total_adjourned']/adjourn_proportion['total_adjournable'])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/home/fiend/Documents/coa/CoA/processed-coa-23-24.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['court'] == 'Kisumu Court of Appeal_Court of Appeal') & (df['outcome'] == \"Case Registered/Filed\")].resample('QE', on='activity_date')['case_type'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kisumu_appeals = df[(df['court'] == 'Kisumu Court of Appeal_Court of Appeal') & (df['case_type'] =='COA  Criminal Appeal') & (df['concluded'] == 1)]\n",
    "#.resample('QE', on='activity_date')['case_type'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kisumu_appeals.groupby('case_type')['case_type'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kisumu_appeals.to_csv('/home/fiend/Documents/coa/CoA/kisumu-coa-appeals.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kisumu_resolved_appeals = df[(df['court'] == 'Kisumu Court of Appeal_Court of Appeal') & (df['concluded'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('court')['concluded'].sum().sort_values(ascending=False).reset_index(name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by quarter and sum the adjourned cases\n",
    "total_adjournments_per_quarter = df.resample('QE', on='activity_date')['adjourned'].sum().reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "total_adjournments_per_quarter.columns = ['quarter', 'total_adjournments']\n",
    "\n",
    "print(total_adjournments_per_quarter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.resample('QE', on='activity_date')['adjournable'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.resample('QE', on='activity_date')['adjourned'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mombasa = df[df['court'] == 'Mombasa Court of Appeal_Court of Appeal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mombasa.groupby('case_type')['concluded'].sum().sort_values(ascending=False).reset_index(name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mombasa.groupby(['caseid_type','broad_case_type'])['concluded'].sum().sort_values(ascending=False).reset_index(name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mombasa['max_resolved'] = mombasa.groupby('case_number')['concluded'].transform('max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mombasa_resolved = mombasa[mombasa['max_resolved'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find total duplicates of mombasa_resolved['case_number'] \n",
    "\n",
    "resolved_cases=mombasa_resolved.groupby(['case_number', 'outcome'])['concluded'].sum().sort_values(ascending=False).reset_index(name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mombasa = mombasa.sort_values(by=['activity_date', 'case_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mombasa[mombasa['case_number']=='Mombasa Court of Appeal_Court of Appeal/COACAPPL/55/2020-10-20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mombasa[mombasa['case_number']=='Mombasa Court of Appeal_Court of Appeal/COAEPA/E002/2023-03-28']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mombasa.groupby('outcome')['concluded'].sum().sort_values(ascending=False).reset_index(name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mombasa.groupby(pd.Grouper(key='activity_date', freq='QE'))['concluded'].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjourned_court = df.groupby(['court', 'date_mon'])['total_adjournable'].sum().reset_index(name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = f\"{file_path}\\\\reports\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "column_order = ['COA  Criminal Appeal', 'Criminal Applications', 'Civil Appeal', 'Civil Applications']\n",
    "\n",
    "filed_cases.to_csv(f'{output_path}filed_cases.csv', index=True)\n",
    "resolved_cases.to_csv(f'{output_path}/resolved_cases.csv', index=True)\n",
    "monthly_filed_cases.to_csv(f'{output_path}filed_per_month.csv', index=False)\n",
    "monthly_concluded_cases.to_csv(f'{output_path}monthly_concluded.csv', index=False)\n",
    "court_productivity.to_csv(f'{output_path}court_productivity.csv', index=True)\n",
    "average_time_to_conclude.to_csv(f'{output_path}average_time_to_conclude.csv', index=True)\n",
    "resolved_within_pmmu_timeline.to_csv(f'{output_path}resolved_within_pmmu_timeline.csv', index=True)\n",
    "#judge_productivity.to_csv(f'{file_path}/reports/judge_productivity.csv', index=False)\n",
    "adjourned_per_court.to_csv(f'{output_path}adjourn_per_court.csv', index=False)\n",
    "adjourn_proportion.to_csv(f'{output_path}adjourn_proportion.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'{file_path}/reports/coa_raw.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine Productivity per judge (Merit/Non Merit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby('judge_1')['concluded'].sum().sort_values(ascending=False).reset_index(name='count').to_csv(f'{file_path}/reports/judge_productivity.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#judges = df['judge_1'].unique().tolist()\n",
    "#judge_productivity = df.groupby('judge_1')['concluded'].sum().sort_values(ascending=False).reset_index(name='count')\n",
    "#judge_productivity.to_csv(f'{output_path}/judge_productivity.csv', index=False)\n",
    "#judge_df.groupby(['judge_1', 'court'])['court'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # productivity per court\n",
    "# court_productivity = df.pivot_table(index='court', columns='productivity', values='concluded', aggfunc='sum', fill_value=0)\n",
    "# court_pivot = court_productivity.rename_axis(columns=None)\n",
    "# # matters handled by judge per court\n",
    "# matters_handled = judge_df.groupby(['court','judge_1'])['court'].count().reset_index(name='count')\n",
    "# # Resolved cases by judge per court\n",
    "# judge_court_productivity = judge_df.groupby(['judge_1', 'court'])['concluded'].sum().reset_index(name='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pending Cases Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_df_cases = merged_df[merged_df['resolved'] != 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_df = unique_df_cases.drop_duplicates(subset='case_number', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_df = unique_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to get only the rows that are in pending_baseline but not in df\n",
    "only_in_baseline = unique_df[unique_df['_merge'] == 'left_only']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_in_baseline.groupby('court').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_dataframe(df):\n",
    "#     # Sort DataFrame by 'activity_date' in descending order\n",
    "#     df_sorted = df.sort_values(by=['court', 'year_filed'], ascending=False)\n",
    "    \n",
    "#     # Drop duplicates based on 'number_on_file' while keeping the first occurrence\n",
    "#     df_unique_cases = df_sorted.drop_duplicates(subset='case_number', keep='first')\n",
    "    \n",
    "#     # Filter unresolved cases\n",
    "#     unique_unresolved_cases = df_unique_cases[df_unique_cases['resolved'] == 0]\n",
    "    \n",
    "#     # Reset index\n",
    "#     unique_unresolved_cases = unique_unresolved_cases.reset_index(drop=True)\n",
    "    \n",
    "#     return unique_unresolved_cases\n",
    "# processed_df = preprocess_dataframe(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_df.groupby('court')['court'].value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.groupby('court')['court'].value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = ['court', 'comingfor', 'outcome', 'activity_date', 'filed_date', 'activity_date_year', 'activity_date_month', 'nature', 'case_category', 'case_number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert both dataframes to sets of 'case_number' values\n",
    "pending_baseline_cases = set(pending_baseline['case_number'].unique())\n",
    "\n",
    "df_cases = set(df['case_number'].unique())\n",
    "\n",
    "# Find the cases present in df but not in df_a\n",
    "cases_not_in_pending = df_cases.difference(pending_baseline_cases)\n",
    "\n",
    "# Print the cases\n",
    "print(\"Cases present in df but not in df_a:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_not_in_pending = pd.DataFrame(cases_not_in_pending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.merge(df, pending_baseline, on='case_number', how='left', indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df[combined_df['_merge'] == 'left_only'].drop(columns='_merge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.sort_values(by=['court', 'activity_date', 'case_number'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert df_a['case_number'] to a set\n",
    "df_a_cases = set(pending_baseline['case_number'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_closed = df[(df['concluded'] == 1) & (df['case_number'].isin(df_a_cases))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter df based on 'outcome' == 1 and 'case_number' not in df_a_cases\n",
    "filtered_df = df[(df['concluded'] == 1) & ~(df['case_number'].isin(df_a_cases))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.groupby('court')['case_number'].count().sort_values(ascending=False).reset_index(name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['resolved'] = filtered_df.groupby('case_number')['concluded'].transform('max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Drop duplicates based on 'number_on_file' while keeping the first occurrence\n",
    "filtered_df = filtered_df.drop_duplicates(subset='case_number', keep='first')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nakuru = filtered_df[filtered_df['court'] == 'Milimani Anti Corruption and Economic Crimes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nakuru.to_csv(f'{output_path}/nakuru.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pending_baseline.to_csv(f'{output_path}/pending_baseline_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.groupby('court')['case_number'].count().sort_values(ascending=False).reset_index(name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pending_baseline[pending_baseline['case_number'] == 'Busia/HCCCMISC/E026/2023']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backlog determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "# raw data of pending cases(next period baseline)\n",
    "#df = pd.read_csv(f'{output_path}/raw_pending_cases.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The date for computation of backlog ought to be the end of the quarter\n",
    "quarter_end = pd.to_datetime('2024-03-31')\n",
    "df['end_date'] = quarter_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to categorize ages\n",
    "def categorize_age(age):\n",
    "    if age <= 365:  # 0-1 years\n",
    "        return '0-1 years'\n",
    "    elif age <= 3 * 365:  # 1-3 years\n",
    "        return '1-3 years'\n",
    "    else:\n",
    "        return 'Over 3 years'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['filed_date'] = pd.to_datetime(df['filed_date'], format='%Y-%m-%d')\n",
    "processed_df['pending_age'] = (processed_df['end_date'] - processed_df['filed_date']).dt.days\n",
    "processed_df = processed_df[processed_df['pending_age'] >= 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df['age_group'] = processed_df['pending_age'].apply(categorize_age)\n",
    "backlog_category = processed_df.pivot_table(index='court', columns='age_group', values='unique_number', aggfunc='count', fill_value=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_order = ['Murder', 'Criminal Appeal', 'Criminal Application', \n",
    "                'Criminal Revision', 'Civil Suit', 'Civil Appeal', 'Miscellaneous Application', \n",
    "                'Constitution Petition', 'Judicial Review',\n",
    "                  'Bankruptcy and Insolvency', 'Tax Appeal', \n",
    "                  'Adoption', 'Divorce', 'Probate Administratio']\n",
    "# Total missing per month\n",
    "missing_per_month.to_csv(f'{output_path}/missing_outcomes_per_month.csv', index=False)\n",
    "\n",
    "# Total filed cases\n",
    "filed_cases.to_csv(f'{output_path}/filed_cases.csv', columns=column_order, index=True)\n",
    "\n",
    "# Total concluded cases\n",
    "concluded_cases.to_csv(f'{output_path}/concluded_cases.csv', columns=column_order, index=True)\n",
    "\n",
    "# Total filed, concluded and CCR per month\n",
    "monthly_cases.to_csv(f'{output_path}/monthly_cases.csv', index=False)\n",
    "\n",
    "# Productivity per court\n",
    "df_pivot.to_csv(f'{output_path}/productivity.csv', index=True)\n",
    "\n",
    "# Average time to conclude\n",
    "average_time_to_conclude.to_csv(f'{output_path}/average_time_to_conclude.csv', index=True)\n",
    "\n",
    "# Time lines\n",
    "proportion_resolved_within_timeline_per_court.to_csv(f'{output_path}/proportion_resolved_within_timeline.csv', index=True)\n",
    "\n",
    "judge_pivot.to_csv(f'{output_path}/judge_productivity.csv', index=True)\n",
    "\n",
    "judge_court_productivity.to_csv(f'{output_path}/judge_court_productivity.csv', index=False)\n",
    "\n",
    "court_pivot.to_csv(f'{output_path}/court_productivity.csv', index=True)\n",
    "\n",
    "matters_handled.to_csv(f'{output_path}/judge_matters_handled.csv', index=False)\n",
    "\n",
    "adjourned_per_court.to_csv(f'{output_path}/adjourned_per_court.csv', index=False)\n",
    "\n",
    "adjourn_proportion.to_csv(f'{output_path}/adjourn_proportion.csv', index=False)\n",
    "\n",
    "pending_cases.to_csv(f'{output_path}/hc_pending_cases.csv', index=True)\n",
    "\n",
    "backlog_category.to_csv(f'{output_path}/hc_backlog.csv', index=True)\n",
    "\n",
    "\n",
    "# raw data of pending cases(next period baseline)\n",
    "processed_df.to_csv(f'{output_path}/raw_pending_cases.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_resolved_within_timeline_per_court.to_csv(f'{output_path}/proportion_resolved_within_timeline.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COA Saving\n",
    "output_path = '/home/arch/devel/data/Report'\n",
    "'''\n",
    "column_order = ['murder', 'criminal_appeal', 'criminal_application', \n",
    "                'revision', 'suit', 'civil_appeal', 'misc_application', \n",
    "                'constitutional_petition', 'judicial_review',\n",
    "                  'bankruptcy_and_insolvency', 'tax_appeal', \n",
    "                  'adoption', 'divorce', 'probate_and_admin']\n",
    "\n",
    "'''\n",
    "column_order  = ['COA Criminal Appeal', 'Criminal Applications', 'Civil Appeal', 'Civil Applications']\n",
    "# Total missing per month\n",
    "missing_per_month.to_csv(f'{output_path}/missing_outcomes_per_month.csv', index=False)\n",
    "\n",
    "# Total filed cases\n",
    "filed_cases.to_csv(f'{output_path}/filed_cases.csv', index=True)\n",
    "\n",
    "# Total concluded cases\n",
    "concluded_cases.to_csv(f'{output_path}/concluded_cases.csv', index=True)\n",
    "\n",
    "# Total filed, concluded and CCR per month\n",
    "monthly_cases.to_csv(f'{output_path}/monthly_cases.csv', index=False)\n",
    "\n",
    "# Productivity per court\n",
    "df_pivot.to_csv(f'{output_path}/productivity.csv', index=True)\n",
    "\n",
    "# Average time to conclude\n",
    "average_time_to_conclude.to_csv(f'{output_path}/average_time_to_conclude.csv', index=True)\n",
    "\n",
    "# Time lines\n",
    "proportion_resolved_within_timeline_per_court.to_csv(f'{output_path}/proportion_resolved_within_timeline.csv', index=True)\n",
    "\n",
    "\n",
    "court_pivot.to_csv(f'{output_path}/court_productivity.csv', index=True)\n",
    "\n",
    "\n",
    "adjourned_per_court.to_csv(f'{output_path}/adjourned_per_court.csv', index=False)\n",
    "\n",
    "adjourn_proportion.to_csv(f'{output_path}/adjourn_proportion.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_resolved_within_timeline_per_court.to_csv(f'{output_path}/proportion_resolved_within_timeline.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'{output_path}/coa_cases.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cases_per_quarter(df, column):\n",
    "    # Group by quarters and count cases\n",
    "    cases_per_quarter = df.groupby(pd.Grouper(key='activity_date', freq='QE'))[column].sum()\n",
    "\n",
    "    # Reset index to make the quarters a column\n",
    "    cases_per_quarter = cases_per_quarter.reset_index()\n",
    "\n",
    "    # Rename the columns\n",
    "    cases_per_quarter.columns = ['quarter', f'cases_{column}']\n",
    "\n",
    "    return cases_per_quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_case_nature_per_quarter(df: pd.DataFrame, column: str, nature: str):\n",
    "   \n",
    "    # Set up date range for quarters\n",
    "    quarters = df['activity_date'].dt.to_period('Q')\n",
    "\n",
    "    # Filter cases where concluded == 1 and case nature == 'civil'\n",
    "    filtered_df = df[(df[column] == 1) & (df['nature'] == nature)]\n",
    "\n",
    "    # Group by quarter and count cases\n",
    "    cases_per_quarter = filtered_df.groupby(quarters).size()\n",
    "\n",
    "    return cases_per_quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop if activity_date is null\n",
    "df = df.dropna(subset=['activity_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterly_adjourned =  get_cases_per_quarter(df, 'adjourned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterly_adjournable =  get_cases_per_quarter(df, 'adjournable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterly_adjournable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterly_concluded =  get_cases_per_quarter(df, 'concluded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterly_concluded =  get_cases_per_quarter(df, 'concluded')\n",
    "quarterly_registered =  get_cases_per_quarter(df, 'registered')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge quarterly_concluded and quarterly_registered on quarter column\n",
    "merged_quarterly = pd.merge(quarterly_concluded, quarterly_registered, on='quarter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('court', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['court', 'month'])['concluded'].sum().reset_index(name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterly_concluded_civil = get_case_nature_per_quarter(df, 'concluded', 'Civil')\n",
    "quarterly_registered_civil = get_case_nature_per_quarter(df, 'registered', 'Civil')\n",
    "\n",
    "quarterly_concluded_criminal = get_case_nature_per_quarter(df, 'concluded', 'Criminal')\n",
    "quarterly_registered_criminal = get_case_nature_per_quarter(df, 'registered', 'Criminal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge quarterly_concluded_civil quarterly_registered_civil quarterly_concluded_criminal quarterly_registered_criminal on quarter column \n",
    "merged_civil_criminal = pd.merge(quarterly_concluded_civil, quarterly_registered_civil, on='quarter')\n",
    "merged_civil_criminal = pd.merge(merged_civil_criminal, quarterly_concluded_criminal, on='quarter')\n",
    "merged_civil_criminal = pd.merge(merged_civil_criminal, quarterly_registered_criminal, on='quarter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_civil_criminal.to_csv(f'{output_path}/quarterly_case_nature.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "### adjourn by event\n",
    "### adjourn by case type\n",
    "\n",
    "### backlog by case type\n",
    "### check if there a courts that resolved more cases than pending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[(df['court'] == 'Milimani Commercial and Tax') & (df['activity_date_year'] == 2024)].groupby('judge_1').size().reset_index(name='count')\n",
    "# group by case_type if outcome == 'Ruling Delivered- Case Closed' \n",
    "#df[df['productivity'] == 'judgment'].groupby('case_category').size().reset_index(name='count')\n",
    "#court_productivity = df[df['productivity'] == 'ruling']\n",
    "#court_productivity_pivot = court_productivity.pivot_table(index='court', columns='case_category', values='concluded', aggfunc='sum', fill_value=0)\n",
    "#productivity_pivot = court_productivity_pivot.rename_axis(columns=None)\n",
    "#df[df['productivity'] == 'ruling'].groupby('case_category').size().reset_index(name='count')\n",
    "#df[df['outcome'] == 'Ruling Delivered- Accused Discharged'].groupby(['court', 'case_category'])['court'].size().reset_index(name='count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pending_count = pending_df.groupby(['court', 'case_category']).size().reset_index(name='pending_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pending_pivot = pending_count.pivot_table(index='court', columns='case_category', values='pending_count', fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pending_pivot.to_csv('/home/arch/devel/data/pending_case_types.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_concluded_cases(df):\n",
    "    \"\"\"\n",
    "    Categorize concluded cases into '1-3 year' and 'over 3 year' based on age column.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame containing the data.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with an additional column 'age_category'.\n",
    "    \"\"\"\n",
    "    # Filter only concluded cases\n",
    "    df_concluded = df[df['concluded'] == 1]\n",
    "\n",
    "    # Calculate age in years\n",
    "    df_concluded['age_years'] = df_concluded['age'] / 365\n",
    "\n",
    "    # Categorize based on age\n",
    "    df_concluded.loc[:, 'age_category'] = df_concluded['age_years'].apply(lambda x: '1-3 year' if 1 <= x <= 3 else 'over 3 year')\n",
    "\n",
    "    # Merge back to original DataFrame\n",
    "    df = pd.merge(df, df_concluded[['case_number', 'age_category']], on='case_number', how='left')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = categorize_concluded_cases(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby judge_1 if court == 'Milimani Commercial and Tax' \n",
    "#df(df['court'] == 'Milimani Commercial and Tax').groupby([ 'judge_1',])['outcome'].size().reset_index(name='count')\n",
    "df[df['court'] == 'Meru'].groupby(['judge_1'])['outcome'].size().reset_index(name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by court if case_type == 'Murder Case' and registered == 1\n",
    "df[(df['case_type'] == 'Criminal Appeal') & (df['outcome'] == 'File Transferred')].groupby('court').size().reset_index(name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['comingfor', 'outcome',  'male_applicant', 'female_applicant',\n",
    "       'organization_applicant', 'male_defendant', 'female_defendant',\n",
    "       'organization_defendant', 'legalrep', 'court',\n",
    "       'activity_date', 'filed_date', 'activity_date_year',\n",
    "       'activity_date_month', 'nature', 'case_category', 'case_number',\n",
    "       'registered', 'concluded', 'productivity', 'age', 'time_lines',\n",
    "       'adjourned', 'adjournable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   # Drop duplicates based on 'number_on_file' while keeping the first occurrence\n",
    "df_unique_cases = df.drop_duplicates(subset='case_number', keep='last')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_cases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filed_df = df[df['registered'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_applicants = filed_df[(filed_df['male_applicant'] == 1) & (filed_df['case_category'] != 'murder')].groupby('case_category').size().reset_index(name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_applicants = filed_df[(filed_df['female_applicant'] == 1) & (filed_df['case_category'] != 'murder')].groupby('case_category').size().reset_index(name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_accused = filed_df[(filed_df['male_defendant'] == 1) & (filed_df['case_category'] == 'murder')].groupby('case_category').size().reset_index(name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_murder_defendants = filed_df[(filed_df['male_defendant'] == 1) & (filed_df['case_category'] == 'murder')].groupby('case_category').size().reset_index(name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_murder_defendants = filed_df[(filed_df['female_defendant'] == 1) & (filed_df['case_category'] == 'murder')].groupby('case_category').size().reset_index(name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applicants = pd.merge(female_applicants, male_applicants, on='case_category', how='outer', suffixes=('_female', '_male'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applicants.to_csv(f'{output_path}/applicants_gender.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "bplot = ax.boxplot(pending_baseline['total_events'],\n",
    "                     vert=False,  # Set vert to False for a vertical box plot\n",
    "                     patch_artist=True) \n",
    "\n",
    "# Add labels for median, mean, etc.\n",
    "# Add labels for median, lower quartile, upper quartile, and outliers\n",
    "for line in bplot.keys():\n",
    "    if line == 'medians':\n",
    "        for median in bplot[line]:\n",
    "            ax.text(median.get_xdata()[0], median.get_ydata()[0],\n",
    "                    f'{median.get_xdata()[0]:.1f}', ha='center', va='bottom', color='red', fontsize=10)\n",
    "    elif line == 'whiskers':\n",
    "        for whisker in bplot[line]:\n",
    "            ax.text(whisker.get_xdata()[0], whisker.get_ydata()[0],\n",
    "                    f'{whisker.get_xdata()[0]:.1f}', ha='center', va='bottom', color='green', fontsize=10)\n",
    "            ax.text(whisker.get_xdata()[1], whisker.get_ydata()[1],\n",
    "                    f'{whisker.get_xdata()[1]:.1f}', ha='center', va='bottom', color='green', fontsize=10)\n",
    "    elif line == 'fliers':\n",
    "        for fliers in bplot[line]:\n",
    "            for f in fliers.get_xdata():\n",
    "                ax.text(f, fliers.get_ydata()[0], f'{f:.1f}', ha='center', va='bottom', color='blue', fontsize=10)\n",
    "\n",
    "\n",
    "ax.set_title('Workload Analysis')\n",
    "ax.set_xlabel('Cases per Judge')\n",
    "\n",
    "# Set face color for the box plot\n",
    "colors = ['lightgreen']\n",
    "for patch, color in zip(bplot['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "data = [pending_baseline['total_events']]\n",
    "\n",
    "# Create a box plot with horizontal orientation\n",
    "fig, ax = plt.subplots()\n",
    "bplot = ax.boxplot(data, vert=True, patch_artist=True)\n",
    "\n",
    "# Add labels for median, mean, etc.\n",
    "for line in bplot.keys():\n",
    "    if line == 'medians':\n",
    "        for median in bplot[line]:\n",
    "            # Add label for median\n",
    "            ax.text(median.get_xdata()[0], median.get_ydata()[0],\n",
    "                    f'{median.get_xdata()[0]:.2f}', ha='center', va='bottom', color='red', fontsize=8)\n",
    "    elif line == 'fliers':\n",
    "        for fliers in bplot[line]:\n",
    "            # Add labels for outliers\n",
    "            for f in fliers.get_xdata():\n",
    "                ax.text(f, fliers.get_ydata()[0], f'{f:.2f}', ha='center', va='bottom', color='blue', fontsize=8)\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Workload')\n",
    "ax.set_ylabel('Values')\n",
    "ax.set_title('Workload Analysis')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "haki-data-HVHT8HQl-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
